---
title: 01 - 人工智能 AI 进阶 --黑马
date: 2024-01-20
categories:
  - 人工智能&大数据
tags:
  - 人工智能
published: false
---
# 0 参考资料

## 0.1 参考书籍
1. 《统计学习方法》-- 李航
2. 《机器学习》-- 周志华
 
## 0.2 参考课程

1. [《人工智能AI进阶》--黑马程序员](https://www.itheima.com/course/pythonaitext.html)
2. [《AI大模型实战训练营》--尚硅谷](http://www.atguigu.com/ai/)

# 1 深度学习基础

## 1.1 初识机器学习
所谓模型，就是一个**包含了大量未知参数的函数**，所谓训练，就是**通过大量的数据去迭 代逼近这些未知参数的最优解**。

**机器学习**： 是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。简单的说，就是”从样本中学习的智能程序“。
**深度学习**：深度学习的概念源于人工神经网络的研究，是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。
不论是机器学习还是深度学习，都是通过对大量数据的学习，掌握数据背后的分布规律进而对符合该分布的其他数据进行准确预测。

机器学习中的典型任务类型可以分为**分类任务**（Classification）和**回归任务**（Regression）。

简单的理解，分类任务是对离散值进行预测，根据每个样本的值/特征预测该样本属于类型A、类型B 还是类型C，例如情感分类、内容审核，相当于学习了一个分类边界 (决策边界)，用分类边界把不同类别的数据区分开来
回归任务是对连续值进行预测，根据每个样本的值/特征预测该样本的具体数值，例如房价预测，股票预测等，相当于学习到了这一组数据背后的分布，能够根据数据的输入预测该数据的取值。

对于分类问题，目的是寻找决策边界其输出空间B不是度量空间，即“定性”。也就是说，在分类问题中，只有分类“正确”与“错误”之分，至于分类到了类别A还是类别B没有分别，都是错误数量+1。
对于回归问题，目的是寻找最优拟合其输出空间B是一个度量空间,，即“定量通过“误差大小”。当真实值为10，预测值为5时，误差度量空间衡量预测值与真实值之间的为5，预测值为8时，误差为2。

## 1.2 机器学习分类

1. 监督学习：监督学习利用大量的标注数据来训练模型，对模型的预测值和数据的真实标签计算损失，然后将误差进行反向传播 (计算梯度、更新参数)，通过不断的学习，最终可以获得识别新样本的能力。
> 每条数据都有正确答案，通过模型预测结果与正确答案的误差不断优化模型参数

2. 无监督学习：无监督学习不依赖任何标签值，通过对数据内在特征的挖掘，找到样本间 的关系，比如聚类相关的任务。有监督和无监督最主要的区别在于模型在训练时是否需 要人工标注的标签信息。
> 只有数据没有答案，常见的是聚类算法，通过衡量样本之间的距离来划分类别

3. 半监督学习：利用有标签数据和无标签数据来训练模型。一般假设无标签数据远多于有标签数据。例如使用有标签数据训练模型，然后对无标签数据进行分类，再使用正确分类的无标签数据训练模型;
> 利用大量的无标注数据和少量有标注数据进行模型训练

4. 自监督学习：机器学习的标注数据源于数据本身，而不是由人工标注。目前主流大模型的预训练过程都是采用自监督学习，将数据构建成完型填空形式，让模型预测对应内容实现自监督学习。
> 通过对数据进行处理，让数据的一部分成为标签，由此构成大规模数据进行模型训练

5. 远程监督学习：主要用于关系抽取任务，采用bootstrap的思想通过已知三元组在文本中寻找共现句，自动构成有标签数据，进行有监督学习。
> 基于现有的三元组收集训练数据，进行有监督学习

6. 强化学习：强化学习是智能体根据已有的经验，采取系统或随机的方式，去尝试各种可能答案的方式进行学习，并且智能体会通过环境反馈的奖赏来决定下一步的行为，并为了获得更好的奖赏来进一步强化学习
> 以获取更高的环境奖励为目标优化模型
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240120140420.png)

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240120140321.png)

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240120140505.png)

## 1.3 模型训练的基本概念
### 1.3.1 基础名词

- 样本：一条数据
- 特征：被观测对象的可测量特性，例如西瓜的颜色、瓜蒂、纹路、敲击声等
- 特征向量：用一个 d 维向量表征一个样本的所有或部分特征
- 标签(label)/真实值：样本特征对应的真实类型或者真实取值，即正确答案
- 数据集(dataset)：多条样本组成的集合
- 训练集(train)：用于训练模型的数据集合
- 评估集(eval)：用于在训练过程中周期性评估模型效果的数据集合
- 测试集(test)：用于在训练完成后评估最终模型效果的数据集合
- 模型：可以从数据中学习到的，可以实现特定功能/映射的函数
- 误差/损失：样本真实值与预测值之间的误差
- 预测值：样本输入模型后输出的结果
- 模型训练：使用训练数据集对模型参数进行迭代更新的过程
- 模型收敛：任意输入样本对应的预测结果与真实标签之间的误差稳定
- 模型评估：使用测试数据和评估指标对训练完成的模型的效果进行评估的过程
- 模型推理/预测：使用训练好的模型对数据进行预测的过程
- 模型部署：使用服务加载训练好的模型，对外提供推理服务

### 1.3.2 模型训练基本流程

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240120135258.png)

机器学习工作流程总结：
1. 获取数据：数据集中一行数据一般称为一个样本，一列数据一般称为一个特征
	1. 数据集的构成：特征值和目标值
	2. 为了模型的训练和测试，把数据集分为：训练集和测试集
2. 数据基本处理
3. 特征工程：是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。
	1. 特征提取
	2. 特征预处理
	3. 特征降维
4. 机器学习(模型训练）：选择合适的算法对模型进行训练
5. 模型评估：对训练好的模型进行评估

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/Pasted%20image%2020240125102358.png)


![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240125102600.png)

Step：一次梯度更新的过程
Epoch：模型完成一次完整训练集的训练（在现实中可以进行1-3个Epoch，多次训练优化模型）

### 1.3.3 Batch size
Batch size是一次向模型输入的数据数量，Batch size越大，模型一次处理的数据量越大，能够更快的运行完一个Epoch，反之运行完一个Epoch越慢。
由于模型一次是根据一个Batch size 的数量计算Loss，然后更新模型参数，如果Batch size过小，单个Batch可能与整个数据的分布有较大差异，会带来较大的噪声，导致模型难以收敛。
与此同时，Batch size 越大，模型单个 Step 加载的数据量越大，对于 GPU 显存的占用也 越大，当 GPU 显存不够充足的情况下，较大的 Batch size 会导致 OOM，因此，需要针对 实际的硬件情况，设置合理的 Batch size 取值。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240126100616.png)
在合理范围内，更大的 Batch size 能够：
- 提高内存利用率，提高并行化效率；
- 一个 Epoch 所需的迭代次数变少，减少训练时间；
- 梯度计算更加稳定，训练曲线更平滑，下降方向更准，能够取得更好的效果；
对于传统模型，在较多场景中，较小的 Batch size 能够取得更好的模型性能；对于大模 型，往往更大的 Batch size 能够取得更好的性能。

## 1.4 学习率
学习率（Learning Rate，LR）决定了模型参数的更新幅度，学习率越高，模型参数更新越激进，即相同的Loss对模型参数产生的调整幅度越大，反之越小。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202111803.png)
如果学习率太小，会导致网络 loss 下降非常慢，如果学习率太大，参数更新的幅度就非常大，会产生震荡，导致网络收敛到局部最优点，或者 loss 不降反增。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202112224.png)


## 1.5 激活函数

每层的神经元输出都会经过激活函数：
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202112515.png)
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202112432.png)

### 1.5.1 为什么需要激活函数
线性函数是一次函数的别称，非线性函数即函数图像不是一条直线的函数，非线性函数包括指数函数、幂函数、对数函数、多项式函数等基本初等函数以及他们组成的复合函数。
**激活函数是多层神经网络的基础，保证多层网络不退化成线性网络。**
线性模型的表达能力不够，激活函数使得神经网络可以逼近其他的任何非线性函数，使得神经网络应用到更多非线性模型中。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202113804.png)
以上网络中没有激活函数的话，y 的值永远是线性的，无法表达不同的非线性情况。

**一般希望激活函数输出均值为0：**
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202114124.png)
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202114213.png)

### 1.5.2 激活函数 -- sigmoid
- sigmoid函数具有软饱和特性，在正负饱和区的梯度都接近于0，只在0附近有比较好的激活特性
- sigmoid导数值最大0.25，也就是反向传播过程中，每层至少有75%的损失，这使得当sigmoid被用在隐藏层的时候，会导致梯度消失（一般5层之内就会产生）
- 函数输出不以0为中心，也就是输出均值不为0，会导致参数更新效率降低
- sigmoid函数涉及指数运算，导致计算速度较慢
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202114508.png)
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202114531.png)
### 1.5.3 激活函数 -- softmax & tanh
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202114603.png)

### 1.5.4 激活函数 -- ReLU
- ReLU 是一个分段线性函数，因此是非线性函数
- ReLU的发明是深度学习领域最重要的突破之一
- ReLU 不存在梯度消失问题
- ReLU 计算成本低，收敛速度比 sigmoid 快6倍
- 函数输出不以0为中心，也就是输出均值不为0，会导致参数更新频率降低
- 存在 dead ReLU 问题（输入 ReLU 有负值时，ReLU输出为0，梯度在反向传播期间无法流动，导致权重不会更新）
- 
  
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202145012.png)

为解决ReLU 的问题，进行了一些优化和变体，在负值时，增加一些斜率，如下是 Leaky ReLU 和 ELU：
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202145538.png)


### 1.5.5 激活函数 -- Swish
- 参数不变情况下，将模型中ReLU替换为Swish，模型性能提升
- Swish 无上界，不会出现梯度饱和
- Swish有下界，不会出现 dead ReLU问题
- Swish 处处连续可导
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202145938.png)
## 1.6 损失函数
### 1.6.1 介绍

损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度（损失值）的运算函数，它是一个非负实值函数。
损失函数仅用于模型训练阶段，得到损失值后，通过反向传播来更新参数，从而降低预测值与真实值之间的损失值，从而提升模型性能。
整个模型训练过程，就是在通过不断更新参数，使得损失函数不断逼近全局最优点（全局最小值）。
不同类型的任务会定义不同的损失函数，例如回归任务中的 MAE、MSE，分类任中的 交叉熵损失 等。

### 1.6.2 损失函数 -- MSE & MAE
**均方误差（mean squared error，MSE）**，也叫平方损失或 L2 损失，常用在最小二乘法中，它的思想是使得各个训练点到最优拟合线的距离最小（平方和最小）。$$MSE = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2$$
**平均绝对误差（Mean Absolute Error，MAE）**，是所有单个测量值与算术平均值的绝对值的平均，也被称为 L1 loss，常用于回归问题中。$$MSE = \frac{1}{m}\sum_{i=1}^{m}|y_i - \hat{y_i}|$$
### 1.6.3 损失函数 -- 交叉熵损失
log 函数是很多损失函数中的重要组成部分，对于 log 函数，默认的底数 a 是 e，也就是损失函数中使用的 log 函数默认 a > 1。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202152058.png)
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202152352.png)


## 1.7 优化器
优化器就是在深度学习反向传播过程中，指引损失函数的各个参数往正确的方向更新合适的大小，使得更新后各个参数让损失函数值不断逼近全局最小。

### 1.7.1 优化器 -- 梯度下降算法
梯度是一个向量，它的每一个分量都是对一个特定变量的偏导数，每个元素都指示了函数里每个变量的最陡上升方向。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202153606.png)

BGD：批量梯度下降法在全部训练集上计算精确的梯度。
SGD：随机梯度下降法则采样单个样本来估计的当前梯度
mini-batch GD：mini-batch 梯度下降法使用batch的一个子集来计算梯度。
为了获取准确的梯度，批量梯度下降法的每一步都把整个训练集载入进来进行计算，时间花费和内存开销都非常大，无法应用于大数据集、大模型的场景。
随机梯度下降法则放弃了对梯度准确性的追求，每步仅仅随机采样一个样本来估计当前梯度，计算速度快，内存开销小。但由于每步接受的信息量有限，随机梯度下降法对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛得情况。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202154309.png)

> 鉴于 BGD 和 SGD 各自的局限性，目前的训练采用 MIni-Batch GD，每次对 batch_size 的数据进行梯度计算，更新参数。

### 1.7.2 优化器 -- Momentum
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202154827.png)

### 1.7.3 优化器 -- AdaGrad
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202154904.png)

### 1.7.4 优化器 -- RMSprop
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202154933.png)

### 1.7.5 优化器 -- Adam
Adam 方法将惯性保持（动量）和自适应这两个优点集于一身。
Adam 记录梯度的一阶矩（first moment），即过往梯度与当前梯度的平均，这体现了惯性保持：$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t$ 
Adam 还记录梯度的二阶矩（second moment），即过往梯度平方与当前梯度平方的平均，这类似 AdaGrad方法，体现了自适应能力，为不同参数产生自适应的学习速率：$$v_t = \beta_2v_{t-1} + (1 - \beta_2)g_t^2$$
一阶矩和二阶矩采用类似于滑动窗口内求平均的思想进行融合，即当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240202155729.png)

## 1.8 模型评估指标


### 1.7.1 评估指标 -- 分类模型

# 2 深度学习经典模型
## 2.1 

# 3 Transformer



# 模型评估

拟合问题：
1. 欠拟合：学习到的东西太少，模型学习的太过粗糙
2. 过拟合：学习到的东西太多，学到的特征太多，不好泛化

# 机器学习-算法篇
## K 近邻算法

## 线性回归
线性回归(Linear regression)是利用**回归方程(函数)**对**一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。
- 特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240121142137.png)

线性回归当中主要有两种模型，**一种是线性关系，另一种是非线性关系。**


## 逻辑回归

## SVM支持向量
支持向量机寻找的最优分类直线应满足：
1. 该直线分开了两类
2. 该直线最大化间隔（margin）
3. 该直线处于间隔的中间，到所有支持向量距离相等。