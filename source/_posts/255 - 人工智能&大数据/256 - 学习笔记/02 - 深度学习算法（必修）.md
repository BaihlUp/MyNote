---
title: 02 - 深度学习算法（必修）
date: 2024-02-01
categories:
  - 人工智能&大数据
tags:
  - 深度学习
  - 算法
published: false
---
# 0 参考资料

**书籍推荐**
1. [统计学习方法 (第2版)](https://book.douban.com/subject/33437381/)
2. [动手学深度学习（PyTorch版）](https://book.douban.com/subject/36142067/)

**课程推荐**
1. [专为程序员设计的高等数学](https://coding.imooc.com/class/chapter/427.html#Anchor)
2. [程序员的数学基础课](https://time.geekbang.org/column/intro/100021201?tab=catalog)
3. [专为程序员设计的统计课](https://coding.imooc.com/class/chapter/371.html#Anchor)
4. [专为程序员设计的线性代数](https://coding.imooc.com/class/chapter/260.html#Anchor)（课程代码）

> 深度学习中运用到的数学知识是很多，但没必要陷入去研究数学，重在理解，够用即可。

[Markdown数学公式、符号](https://blog.csdn.net/weixin_42782150/article/details/104878759)

**conda工具常用命令：**
- `conda activate env_name`  # 切换至 env_name 环境 
- `conda deactivate`  # 退出环境
- `conda info -e`        # 显示所有已经创建的环境 或者 使用 `conda env list`
- `conda list`              # 查看所有已经安装的包
- `conda install package_name`    # 在当前环境中安装包
- `conda remove package`                # 删除当前环境中的包
- `conda create -n env_name package_name`   # 创建名为 env_name 的新环境，并在该环境下安装名为package_name的包，可以指定新环境的版本号
- `conda remove --name env_name --all`       # 删除环境

**Jupyter-Notebook 常用快捷键：**
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201140256.png)

# 1 数学基础
## 1.1 集合、映射与函数
- 设A、B是两个非空集合，如果存在一个法则f，使得对A中的每个元素a，按法则 f，在B中有唯一确定的元素b与之对应，那么称 f 为从 A 到B的映射，记作 f：A-> B
- b 称为元素a在映射f下的像，记作 b = f(a)
- a 称为元素 b 在映射 f 下的一个原像
- A 称为 映射 f 的定义域，记作 D<sub>f</sub>
- A 中所有元素的像所组成的集合称为映射 f 的值域，记为 R<sub>f</sub>

![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2023/20240201102430.png)

## 1.2 线性代数
 
**标量**：一个标量就是一个数，它只有大小，没有方向 
**向量**：向量是一组标量排列而成，只有一个轴，沿着行或者列的方向。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201103028.png)

**模长和范数**：
	向量的模长：可以简称为向量的模，英文为norm。表示向量在空间的长度。
	对二维向量 a = (a<sub>1</sub>, a<sub>2</sub>)，其模长 ||a|| 等于 $$\sqrt{a^2 + a_2^2}$$
	对n 维向量 a = (a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>n</sub>)，其模长 ||a|| 等于 $$\sqrt{a_1^2 + a_2^2 + ... + a_n^2}$$
	![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201104413.png)

**单位向量：**
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201104512.png)

**向量的内积：** 也称为点乘、点积，是两个向量对应位置元素相乘后相加，结果是一个标量。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201104701.png)

**向量的外积**：又称向量叉积、叉乘等。外积的运算结果是一个向量而不像内积是一个标量。
	两个向量的叉积与这两个向量组成的坐标平面垂直，其值取决于a，b的方向和大小，对应计算公式如下：
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201104900.png)

**矩阵：** 是由多个元素组成的表格，是一种二维数据结构，每个数字在矩阵中都有一个对应的行号和列号。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201105004.png)

**矩阵转置**：矩阵的转置是以主对角线为轴，进行镜像翻转。矩阵转置公式如下：
$$(A)^T_m,_n = A_n,_m$$
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201105315.png)

**矩阵乘法：** 有m行k列的矩阵 A 和 k行 n列的矩阵 B，矩阵A和矩阵B相乘，则 A 的列数 必须和B的行数相乘。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201105433.png)

**矩阵相乘示例：**
$$\left[
\begin{matrix}
1 & 2 & 3 & 4\\
1 & 1 & 1 & 1 \\
5 & 6 & 7 & 8 
\end{matrix}
\right]

\times

\left[
\begin{matrix}
1 & 5 \\
2 & 6 \\
3 & 7 \\
4 & 8
\end{matrix}
\right]

=

\left[
\begin{matrix}
30 & 70 \\
10 & 26 \\
70 & 174 
\end{matrix}
\right]

$$

**矩阵内积：** 结果是一个标量，等于两个矩阵AB对应元素直接相乘再相加。对应公式如下：
$$ c = \sum_{i=1}^{m}\sum_{j=1}^{n}A_{i,j}B_{i,j}$$
**哈达玛积（Hadamard product）：** 两个矩阵AB对应元素直接相乘，结果是一个矩阵：
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201111214.png)

**矩阵乘法的性质：**
- 交换律：$AB \leq BA$
- 分配律：$A(B + C) = AB + AC$
- 结合律：$(AB)C = A(BC)$
- 转置性质：$(AB)^T = B^TX^T$

**张量：** 是多维数组的抽象概括，可以看作是向量和矩阵的推广。向量矩阵的运算方法对张量同样适用。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201111732.png)

## 1.3 微积分
**极限：** 表示某一点处函数值趋近于某一特定值的过程，一般记为：$\lim_{x\rightarrow{a}}f(x) = L$ ，极限是一种变化状态的描述，核心思想是无限靠近而永远不能到达。

**导数：** 是函数的局部性质，指一个函数在某一点附近的变化率，对函数 $y=f(x)$ 来说，他的导数用符号 $f^{\prime}(x)$ 来表示，也可记为 $\frac{df(x)}{dx}$ 。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201112939.png)

**常见导数计算公式：**
- 常数函数：   $f(x) = C$         $f^{\prime}(x) = 0$
- 幂函数： $f(x) = x^n$           $f^{\prime}(x) = nx^{n-1}$
- 指数函数：$f(x) = e^x$           $f^{\prime}(x) = e^x$
- 对数函数：$f(x) = ln(x)$      $f^{\prime}(x) = \frac{1}{x}$

**微分：** 是指对函数的局部变化的一种线性描述，自变量的微分记作 $dx$ ，函数 $y = f(x)$ 的微分记作 $dy = df(x) = f^{\prime}(x)dx$
	导数是微分的比值：$f^{\prime} = \frac{df(x)}{dx}$
	导数表示变化率，微分表示变化量

**偏导数**：指的是多元函数在某一点处关于某一变量的导数。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201114014.png)

**梯度：** 为一个包含所有偏导数的向量，符号是 $\nabla$
对函数 $z = f(x,y) = x^2 + y^2$ 来说，它的梯度向量是：$\nabla f(x,y) = (2x,2y)$
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201122852.png)

**链式法则：** 用来计算复合函数导数的。假设对实数x，有可微函数 f 和 g，其中 $z = f(y), y = g(x)$ 那么，链式法则公式如下：$$\frac{dz}{dx}=\frac{dz}{dy}.\frac{dy}{dx}$$
> 所谓链式法则，就是一层一层增加可以“相互抵消”的分子分母

有函数 $f(x)=x^2$ 和 $g(x)=x+1$ ，计算 $h(x)=f(g(x))=(x+1)^2$ 的导数，可得：$$h^\prime(x) = f^\prime(g(x)){\cdot}g^\prime(x) = 2(x+1){\cdot}1 = 2x + 2$$

## 1.4 概率统计
**概率：**
- 概率可以用来表示模型的准确率（错误率）
- 概率可以用来描述模型的不确定性
- 概率可以作为模型损失的度量

**事件：**
- 事件相当于实验的结果
- 随机事件指一次或多次随机试验的结果
- 事件的基本属性包括：可能性、确定性、兼容性
- 依赖事件指的是事件的发生受其他事件影响
- 独立事件指的是事件的发生与其他事件无关

**随机变量和概率分布：**
- 随机变量是概率统计中用来表示随机事件结果的变量
- 随机变量包括离散型随机变量和连续型随机变量
- 概率分布用来描述随机变量的分布情况

**概率密度：**
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201124520.png)

**联合概率和条件概率：**
- 联合概率指同时发生两个或多个事件的概率，记为 $P(A,B)$
- 条件概率是指在某个条件下发生某个事件的概率，记为 $P(A|B)$
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201124936.png)
- 联合概率和条件概率相互转化： $P(A,B) = P(A|B)P(B)$        $P(A|B) = \frac{P(A,B)}{P(B)}$

**贝叶斯定理：** 表示在已知条件概率的情况下，可以推导出联合概率。常用于根据已知信息推测未知信息的场景，公式如下：$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
**极大似然估计：** Maximum Likelihood Estimation，MLE：利用已知的样本结果，反推最有可能导致这样结果的参数值，即找到参数的最大概率取值。
![](https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240201125514.png)

对于给定的样本集 $X = {x_1,x_2,...,x_n}$ 我们需要估计参数向量 $\theta$，此时可以计算似然函数 $L(\theta)$ ，等于联合概率密度函数 $p(X|\theta)$
公式表示如下：$$L(\theta) = p(X|\theta) = \prod_{i=1}^{n}p(x_i|\theta)$$
# 2 深度神经网络
## 2.1 